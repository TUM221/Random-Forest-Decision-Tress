# -*- coding: utf-8 -*-
"""Decision_Trees_and_Random_Forest_Project_.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hRpD0iatf_GdVCeBwc-yIjdkKQcwoF-C

# Random Forest Project

# Import Libraries

**Import the usual libraries for pandas and plotting.Import sklearn later on.**
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

"""## Get the Data

** Use pandas to read loan_data.csv as a dataframe called loans.**
"""

loans = pd.read_csv('loan_data.csv')

"""** Check out the info(), head(), and describe() methods on loans.**"""

loans.info()

loans.describe()

loans.head()

"""# Exploratory Data Analysis

for data visualization! use seaborn and pandas built-in plotting capabilities

** Create a histogram of two FICO distributions on top of each other, one for each credit.policy outcome.**


"""

plt.figure(figsize=(10,6))
loans[loans['credit.policy']==1]['fico'].hist(alpha=0.5,color='blue',
                                              bins=30,label='Credit.Policy=1')
loans[loans['credit.policy']==0]['fico'].hist(alpha=0.5,color='red',
                                              bins=30,label='Credit.Policy=0')
plt.legend()
plt.xlabel('FICO')

"""** Create a similar figure, except this time select by the not.fully.paid column.**"""

plt.figure(figsize=(10,6))
loans[loans['not.fully.paid']==1]['fico'].hist(alpha=0.5,color='blue',
                                              bins=30,label='not.fully.paid=1')
loans[loans['not.fully.paid']==0]['fico'].hist(alpha=0.5,color='red',
                                              bins=30,label='not.fully.paid=0')
plt.legend()
plt.xlabel('FICO')

"""** Create a countplot using seaborn showing the counts of loans by purpose, with the color hue defined by not.fully.paid. **"""

plt.figure(figsize=(11,7))
sns.countplot(x='purpose',hue='not.fully.paid',data=loans,palette='Set1')

"""** Let's see the trend between FICO score and interest rate. Recreate the following jointplot.**"""

sns.jointplot(x='fico',y='int.rate',data=loans,color='purple')

plt.figure(figsize=(11,7))
sns.lmplot(y='int.rate',x='fico',data=loans,hue='credit.policy',
           col='not.fully.paid',palette='Set1')

"""# Setting up the Data

To set up the data for Random Forest Classification Model!

**Check loans.info() again.**
"""

loans.info()

"""## Categorical Features

Notice that the **purpose** column as categorical


**Create a list of 1 element containing the string 'purpose'. Call this list cat_feats.**
"""

cat_feats = ['purpose']

"""**Now use pd.get_dummies(loans,columns=cat_feats,drop_first=True) to create a fixed larger dataframe that has new feature columns with dummy variables. Set this dataframe as final_data.**"""

final_data = pd.get_dummies(loans,columns=cat_feats,drop_first=True)

final_data.info()

"""## Train Test Split


"""

from sklearn.model_selection import train_test_split

X = final_data.drop('not.fully.paid',axis=1)
y = final_data['not.fully.paid']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=101)

"""## Training a Decision Tree Model

 By training a single decision tree first!

** Import DecisionTreeClassifier**
"""

from sklearn.tree import DecisionTreeClassifier

"""**Create an instance of DecisionTreeClassifier() called dtree and fit it to the training data.**"""

dtree = DecisionTreeClassifier()

dtree.fit(X_train,y_train)

"""## Predictions and Evaluation of Decision Tree
**Create predictions from the test set and create a classification report and a confusion matrix.**
"""

predictions = dtree.predict(X_test)

from sklearn.metrics import classification_report,confusion_matrix

print(classification_report(y_test,predictions))

print(confusion_matrix(y_test,predictions))

"""## Training the Random Forest model


**Create an instance of the RandomForestClassifier class and fit it to training data from the previous step.**
"""

from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier(n_estimators=600)

rfc.fit(X_train,y_train)

"""## Predictions and Evaluation

Let's predict off the y_test values and evaluate our model.

** Predict the class of not.fully.paid for the X_test data.**
"""

predictions = rfc.predict(X_test)

"""**Now create a classification report from the results. Do you get anything strange or some sort of warning?**"""

from sklearn.metrics import classification_report,confusion_matrix

print(classification_report(y_test,predictions))

"""**Show the Confusion Matrix for the predictions.**"""

print(confusion_matrix(y_test,predictions))

"""**What performed better the random forest or the decision tree?**"""

# Depends what metric you are trying to optimize for. 
# Notice the recall for each class for the models.
# Neither did very well, more feature engineering is needed.